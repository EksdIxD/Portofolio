{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries and Dataset"
      ],
      "metadata": {
        "id": "X8TPm3IlBih8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Install Repositories*\n",
        "* https://github.com/marcotcr/lime"
      ],
      "metadata": {
        "id": "jc8b8O23Br2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4Fyn5gpM46b"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install Sastrawi\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Import Related Libraries*"
      ],
      "metadata": {
        "id": "-Upyf-MsBv0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytWvrMiFNSDn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import lime\n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Import Datasets*"
      ],
      "metadata": {
        "id": "lxKwPeHl_BYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z55U2_TiNX8u"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('re_dataset.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Raw tweet dataset."
      ],
      "metadata": {
        "id": "97DWojDdCChW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original',\n",
        "                                      1: 'replacement'})"
      ],
      "metadata": {
        "id": "fFjmUMzEB_CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Dictionary of slang and mistyped words."
      ],
      "metadata": {
        "id": "KuI82crCCGj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_stopword_dict = pd.read_csv('stopwordbahasa.csv', header=None)\n",
        "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})"
      ],
      "metadata": {
        "id": "AIcg4aGKB7YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: Dictionary of stopwords. Taken from (https://www.kaggle.com/datasets/oswinrh/indonesian-stoplist)"
      ],
      "metadata": {
        "id": "ARcCH1ZjCMon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing\n"
      ],
      "metadata": {
        "id": "D3d5gb06CQGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_unnecessary_char(text):\n",
        "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
        "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('user',' ',text) # Remove every username\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "def remove_nonaplhanumeric(text):\n",
        "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
        "def normalize_alay(text):\n",
        "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
        "\n",
        "def remove_stopword(text):\n",
        "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def stemming(text):\n",
        "    return stemmer.stem(text)"
      ],
      "metadata": {
        "id": "3duLIbaiUd2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = lowercase(text) # 1\n",
        "    text = remove_nonaplhanumeric(text) # 2\n",
        "    text = remove_unnecessary_char(text) # 2\n",
        "    text = normalize_alay(text) # 3\n",
        "    #text = stemming(text) # 4\n",
        "    text = remove_stopword(text) # 5\n",
        "    return text"
      ],
      "metadata": {
        "id": "n4exWd7gV3X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZXWEMNyNcIs"
      },
      "outputs": [],
      "source": [
        "data['Tweet'] = data['Tweet'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification and Evaluation**"
      ],
      "metadata": {
        "id": "Ut_hcCf2Rek0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WYakT8ROfGn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(data['Tweet'])\n",
        "y = data['HS']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9Ybis_2uXsF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UbNlIhSW7RuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c8pZvxQCCbbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "996RwZ-4MFVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a3bd39-5e29-47ed-c854-0b96746aa808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.86      1516\n",
            "           1       0.86      0.70      0.77      1118\n",
            "\n",
            "    accuracy                           0.82      2634\n",
            "   macro avg       0.83      0.81      0.81      2634\n",
            "weighted avg       0.83      0.82      0.82      2634\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)"
      ],
      "metadata": {
        "id": "6ZgrFbM_MIxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b1ef57-e648-4409-c724-b8ec184603dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8230827638572513\n",
            "Precision:  0.8268166871498865\n",
            "Recall:  0.8230827638572513\n",
            "F1-score:  0.8198672365659038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet : \", data['Tweet'][1164])\n",
        "\n",
        "if y_pred[1164] == 0:\n",
        "  print(\"The tweet is not a hate speech\")\n",
        "else:\n",
        "  print(\"The tweet is a hate speech\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq_T0Ijme2cw",
        "outputId": "7be8d2db-9209-426c-f57e-8ef457f25d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet :  jokowi staf ahli khusus mempe ahankan aset nasional diktator jomlo lokal\n",
            "The tweet is not a hate speech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tweet : \", data['Tweet'][1062])\n",
        "\n",
        "if y_pred[1062] == 0:\n",
        "  print(\"The tweet is not a hate speech\")\n",
        "else:\n",
        "  print(\"The tweet is a hate speech\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_bD_OsMgfHS",
        "outputId": "d5ce8bb4-d7ea-48ce-d4b6-083af2594870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet :  bodoh bangsa gue ujian semester uniform resource locator\n",
            "The tweet is a hate speech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Convert y_train to numpy array\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "input_dim = X_train.shape[1]  # Replace with the actual number of input features\n",
        "num_classes = len(np.unique(y_train))\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Specify the training parameters\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Shuffle the training data\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    # Mini-batch training\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "        # Perform forward pass and compute loss\n",
        "        loss, accuracy = model.train_on_batch(X_batch, y_batch)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f\"Validation loss: {val_loss:.4f} - Validation accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "WX0Kp_gq-5uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c.predict_proba(data['Tweet'][0]))"
      ],
      "metadata": {
        "id": "IjNhmNtxBOhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTES:\n",
        "* https://www.kaggle.com/code/bavalpreet26/explainable-ai-lime/notebook\n",
        "* https://towardsdatascience.com/lime-how-to-interpret-machine-learning-models-with-python-94b0e7e4432e\n",
        "\n"
      ],
      "metadata": {
        "id": "oH35-n-ENxka"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}