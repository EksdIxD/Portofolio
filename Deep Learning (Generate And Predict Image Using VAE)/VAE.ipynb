{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "from keras.layers import Dense, Input, MaxPool2D, Conv2D, Flatten, InputLayer, Reshape, Conv2DTranspose, BatchNormalization\n",
    "from keras import Sequential\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./dataset\"\n",
    "img_rows, img_cols = 28,28\n",
    "X = []\n",
    "\n",
    "for counter, img_path in enumerate(os.listdir(PATH)):\n",
    "    path = os.path.join(PATH, img_path)\n",
    "    image = load_img(path, target_size=(img_rows, img_cols))\n",
    "    image = img_to_array(image)\n",
    "    X.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngubah X ke array dan normalisasi \n",
    "X = np.array(X, dtype = 'float32') / 255.0\n",
    "X = X.reshape(-1, img_rows, img_cols, 3) # \n",
    "X_train, X_test = train_test_split(X, test_size = 0.4, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model dari VAE\n",
    "hidden_dimension = 512 # Dimension dari input dan output layer\n",
    "latent_dimension = 64 # Latent dimension HARUS lebih kecil dari hidden dimension\n",
    "batch_size = 200\n",
    "learning_rate = 0.00015\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, hidden_dimension, latent_dimension):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        # Encoder Layer\n",
    "        self.encoder = Sequential([\n",
    "            InputLayer(input_shape = (img_rows, img_cols, 3)),\n",
    "            Conv2D(32, kernel_size = (3, 3), activation = 'relu', strides = (2, 2), padding = 'same'),\n",
    "            MaxPool2D((2,2), padding = 'same'),\n",
    "            Conv2D(64, kernel_size = (3, 3), activation = 'relu', strides = (2, 2), padding = 'same'),\n",
    "            Flatten(),\n",
    "            Dense(hidden_dimension, activation = 'relu')\n",
    "        ])\n",
    "\n",
    "        # Latent Layer (2 Layer) --> Mean Layer, Loag Variance Layer\n",
    "        self.mu_layer = Dense(latent_dimension)\n",
    "        self.log_var_layer = Dense(latent_dimension)\n",
    "\n",
    "        # Decoder Layer\n",
    "        self.decoder = Sequential([\n",
    "            Dense(hidden_dimension, activation = 'relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(7 * 7 * 32, activation = 'relu'),\n",
    "            Reshape((7, 7, 32)), \n",
    "            Conv2DTranspose(64, kernel_size = (3, 3), activation = 'relu', strides = (2, 2), padding = 'same'),\n",
    "            Conv2DTranspose(32, kernel_size = (3, 3), activation = 'relu', strides = (2, 2), padding = 'same'),\n",
    "            Conv2DTranspose(3, kernel_size = (3, 3), padding = 'same') # 3 itu warna channel kita (RGB) dan ga dikasih strides lagi karena ukurannya sudah sesuai (7 * 2 * 2 = 28)\n",
    "        ])\n",
    "\n",
    "    # Encode = proses untuk memasukkan input ke layer\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.mu_layer(h), self.log_var_layer(h)\n",
    "    \n",
    "    # Proses Decode dan ini akan dipanggil saat testing\n",
    "    def decode(self, z):\n",
    "        # Activation Function karna udah mau ditunjukin hasilnya (sama kyk training biasa, proses akhir tetap ada activation function sigmoid)\n",
    "        return tf.nn.sigmoid(self.decode_logits(z))\n",
    "\n",
    "    # Decode Logits = 1 step sebelum activation function dalam artian hasilnya udah jadi tapi masih mentah\n",
    "    # Dan ini akan dipanggil saat training\n",
    "    def decode_logits(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    # Trik untuk melakukan backpropagation secara efisien (dengan normal distribusi matematika)\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = tf.exp(log_var * 0.5) # Untuk mencari Standar Deviasi\n",
    "        eps = tf.random.normal(std.shape) # Untuk mendapatkan epsilon\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        mu, log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconstructed_logits  = self.decode_logits(z)\n",
    "        return x_reconstructed_logits, mu, log_var\n",
    "    \n",
    "model = VAE(hidden_dimension, latent_dimension)\n",
    "model.build(input_shape = (batch_size, img_rows, img_cols, 3))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngubah data train ke bentuk object tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "dataset = dataset.shuffle(batch_size).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Result dari loss dan div akan disimpan pada setiap stepsnya \n",
    "loss_history = [] # loss --> mean(reconstruction_loss) + mean(kl_div)\n",
    "klDiv_history = [] # klDiv --> Kullback Leiber Divergence adalah perbedaan probabilitas dari training step sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for step, x in  enumerate(dataset):\n",
    "        with tf.GradientTape() as tape: # Fungsi ini akan secara otomatis me-record perbedaan dari masing - masing kalkulasi \n",
    "            # Calculate Loss Function\n",
    "            x_reconstructed_logits, mu, log_var = model(x)\n",
    "            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = x, logits = x_reconstructed_logits)\n",
    "            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_size\n",
    "\n",
    "            kl_div = -0.5 * tf.reduce_sum((1 + log_var - tf.square(mu) - tf.exp(log_var)), axis = -1)\n",
    "            kl_div = tf.reduce_mean(kl_div)\n",
    "\n",
    "            loss = tf.reduce_mean(reconstruction_loss) + kl_div\n",
    "\n",
    "        # Model.trainable_variables --> variable yang bisa diganti di model tersebut ada apa aja\n",
    "        # Backpropagation --> mengubah weight dari layer (Bias, Weight)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        gradients = [tf.clip_by_norm(g, 5) for g in gradients]\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        loss_history.append(float(reconstruction_loss))\n",
    "        klDiv_history.append(float(kl_div))\n",
    "\n",
    "        print(f'Epoch[{epoch + 1}/{epochs}], Step[{step + 1}/{len(dataset)}], Reconstuction Loss : {float(reconstruction_loss):.2f}, KL Div : {float(kl_div):.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and Prediction the Test data\n",
    "mu, log_var = model.encode(X_test)\n",
    "# Hasil dari generated data kita\n",
    "out = model.decode(mu)\n",
    "out = (tf.reshape(out, [-1, img_rows, img_cols, 3]).numpy() * 255).astype(np.uint8) # Unnormalisasi karena udah mau show hasilnya --> * 255 dan convert ke int biar hasilnya integer ga float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "random_indices = random.sample(range(len(X_test)), num_samples)\n",
    "plt.figure(figsize = (20, 4))\n",
    "\n",
    "# Original Image\n",
    "for i, val in enumerate(random_indices):\n",
    "    plt.subplot(2, num_samples, i + 1)\n",
    "    plt.imshow(X_test[val], cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "for i, val in enumerate(random_indices):\n",
    "    plt.subplot(2, num_samples, i + 11)\n",
    "    plt.imshow(out[val], cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history, label = 'Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(klDiv_history, label = 'Loss')\n",
    "plt.title('Model KL Div')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('KL Div')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
